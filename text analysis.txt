# Text Analytics
# 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words
# removal, Stemming and Lemmatization.
# 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.


import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag


# Example document
document = "This is an example document that we will use to demonstrate document preprocessing."


# Tokenization
tokens = word_tokenize(document)

tokens

import nltk
nltk.download('averaged_perceptron_tagger')

# POS tagging
#These tags can indicate whether a word is a noun, verb, adjective, adverb, preposition, conjunction, or other g
pos_tags = pos_tag(tokens)


pos_tags
# DT for determiner , NN for noun , VBD for past tense word , IN for preprosition
#VBZ: verb, 3rd person singular present tense (e.g. "runs")
# VB: verb, base form (e.g. "run")
# PRP: personal pronoun (e.g. "he", "she", "it", "they")
# MD: modal verb (e.g. "can", "should", "will")
# TO: to (e.g. "to run", "to go")
# Here are some additional tags that you may find useful:
# NN: noun, singular or mass (e.g. "cat", "dog", "water")
# NNS: noun, plural (e.g. "cats", "dogs")
# JJ: adjective (e.g. "happy", "blue")
# RB: adverb (e.g. "quickly", "very")
# IN: preposition or subordinating conjunction (e.g. "in", "on", "because")

# Stopwords removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if not word.lower() in stop_words]

# Stemming
ps = PorterStemmer()
stemmed_tokens = [ps.stem(word) for word in filtered_tokens]


# Lemmatization
wnl = WordNetLemmatizer()
lemmatized_tokens = [wnl.lemmatize(word) for word in filtered_tokens]

# Print the results
print("Tokens: ", tokens)
# print("POS tags: ", pos_tags)
print("Filtered tokens: ", filtered_tokens)
print("Stemmed tokens: ", stemmed_tokens)
print("Lemmatized tokens: ", lemmatized_tokens)
#NLTK is capable of performing all the document preprocessing methods that you have mentioned.

# Text Analytics
# Create representation of document by calculating Term Frequency and Inverse Document Frequency

import math
from collections import Counter

# Sample corpus of documents
corpus = [
 'The quick brown fox jumps over the lazy dog',
 'The brown fox is quick',
 'The lazy dog is sleeping'
]


# Tokenize the documents
tokenized_docs = [doc.lower().split() for doc in corpus]

# Count the term frequency for each document
tf_docs = [Counter(tokens) for tokens in tokenized_docs]

# Calculate the inverse document frequency for each term
n_docs = len(corpus)
idf = {}
for tokens in tokenized_docs:
 for token in set(tokens):
  idf[token] = idf.get(token, 0) + 1
for token in idf:
 idf[token] = math.log(n_docs / idf[token])

# Calculate the TF-IDF weights for each document
tfidf_docs = []
for tf_doc in tf_docs:
 tfidf_doc = {}
 for token, freq in tf_doc.items():
  tfidf_doc[token] = freq * idf[token]
 tfidf_docs.append(tfidf_doc)

# Print the resulting TF-IDF representation for each document
for i, tfidf_doc in enumerate(tfidf_docs):
 print(f"Document {i+1}: {tfidf_doc}")

# This code uses the Counter class from the collections module to count the term frequency for each document. It then
# calculates the inverse document frequency by iterating over the tokenized documents and keeping track of the number of
# documents that each term appears in. Finally, it multiplies the term frequency of each term in each document by its
# corresponding inverse document frequency to get the TF-IDF weight for each term in each document. The resulting TF-IDF
# representation for each document is printed to the console.
